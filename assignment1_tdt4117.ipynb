{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00a8eb",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 1: Boolean Model, TF-IDF, and Data Retrieval vs. Information Retrieval Conceptual Questions\n",
    "\n",
    "**Student names**: _Your_names_here_ <br>\n",
    "**Group number**: _Your_group_here_ <br>\n",
    "**Date**: _Submission Date_\n",
    "\n",
    "## Important notes\n",
    "Please carefully read the following notes and consider them for the assignment delivery. Submissions that do not fulfill these requirements will not be assessed and should be submitted again.\n",
    "1. You may work in groups of maximum 2 students.\n",
    "2. The assignment must be delivered in ipynb format.\n",
    "3. The assignment must be typed. Handwritten assignments are not accepted.\n",
    "\n",
    "**Due date**: 14.09.2025 23:59\n",
    "\n",
    "In this assignment, you will:\n",
    "- Implement a Boolean retrieval model\n",
    "- Compute TF-IDF vectors for documents\n",
    "- Run retrieval on queries\n",
    "- Answer conceptual questions \n",
    "\n",
    "---\n",
    "## Dataset\n",
    "\n",
    "You will use the **Cranfield** dataset, provided in this file:\n",
    "\n",
    "- `cran.all.1400`: The document collection (1400 documents)\n",
    "\n",
    "**The code to parse the file is ready — just update the cran file path to match your own file location. Use the docs variable in your code for the parsed file**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3249058",
   "metadata": {},
   "source": [
    "### Load and parse documents (provided)\n",
    "\n",
    "Run the cell to parse the Cranfield documents. Update the path so it points to your `cran.all.1400` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773d293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1400 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read 'cran.all.1400' and parse the documents into a suitable data structure\n",
    "\n",
    "CRAN_PATH = r\"cran.all.1400\"  # <-- change this!\n",
    "\n",
    "def parse_cranfield(path):\n",
    "    docs = {}\n",
    "    current_id = None\n",
    "    current_field = None\n",
    "    buffers = {\"T\": [], \"A\": [], \"B\": [], \"W\": []}\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\".I \"):\n",
    "                if current_id is not None:\n",
    "                    docs[current_id] = {\n",
    "                        \"id\": current_id,\n",
    "                        \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "                        \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "                    }\n",
    "                current_id = int(line.split()[1])\n",
    "                buffers = {k: [] for k in buffers}\n",
    "                current_field = None\n",
    "            elif line.startswith(\".\"):\n",
    "                tag = line[1:].strip()\n",
    "                current_field = tag if tag in buffers else None\n",
    "            else:\n",
    "                if current_field is not None:\n",
    "                    buffers[current_field].append(line)\n",
    "    if current_id is not None:\n",
    "        docs[current_id] = {\n",
    "            \"id\": current_id,\n",
    "            \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "            \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "        }\n",
    "    print(f\"Parsed {len(docs)} documents.\")\n",
    "    return docs\n",
    "\n",
    "docs = parse_cranfield(CRAN_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8f900",
   "metadata": {},
   "source": [
    "## 1.1 – Boolean Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81516f89",
   "metadata": {},
   "source": [
    "### 1.1.1 Tokenize documents\n",
    "\n",
    "Implement tokenization using the given list of stopwords. Create a list of normalized terms per document (e.g., lowercase, remove punctuation/digits; drop stopwords). Store the token lists to use in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78a135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'title': 'experimental investigation of the aerodynamics of a wing in a slipstream .', 'abstract': 'experimental investigation of the aerodynamics of a wing in a slipstream .   an experimental study of a wing in a propeller slipstream was made in order to determine the spanwise distribution of the lift increase due to slipstream at different angles of attack of the wing and at different free stream to slipstream velocity ratios .  the results were intended in part as an evaluation basis for different theoretical treatments of this problem .   the comparative span loading curves, together with supporting evidence, showed that a substantial part of the lift increment produced by the slipstream was due to a /destalling/ or boundary-layer-control effect .  the integrated remaining lift increment, after subtracting this destalling lift, was found to agree well with a potential flow theory .   an empirical evaluation of the destalling effects was made for the specific configuration of the experiment .'}\n",
      "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement tokenization using the given list of stopwords, create list of terms per document\n",
    "\n",
    "STOPWORDS = set(\"\"\"a about above after again against all am an and any are aren't as at be because been\n",
    "before being below between both but by can't cannot could couldn't did didn't do does doesn't doing don't down\n",
    "during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers\n",
    "herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most\n",
    "mustn't my myself no nor not of off on once only or other ought our ours ourselves out over own same shan't she\n",
    "she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's\n",
    "these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're\n",
    "we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't\n",
    "you you'd you'll you're you've your yours yourself yourselves\"\"\".split())\n",
    "\n",
    "import re\n",
    "from typing import Iterable, List\n",
    "\n",
    "_SANITIZED_STOPWORDS = set()\n",
    "for w in STOPWORDS:\n",
    "    _SANITIZED_STOPWORDS.update(re.sub(r\"[^a-z]+\", \" \", w.lower()).split())\n",
    "\n",
    "def tokenize_documents(docs: Iterable[dict]) -> List[List[str]]:\n",
    "    out = []\n",
    "    for rec in docs:\n",
    "        text = f\"{rec['title']} {rec['abstract']}\".lower()\n",
    "        text = re.sub(r\"[^a-z]+\", \" \", text)   # keep only a–z\n",
    "        tokens = [t for t in text.split() if t and t not in _SANITIZED_STOPWORDS]\n",
    "        out.append(tokens)\n",
    "    return out\n",
    "\n",
    "\n",
    "DOC_IDS = sorted(docs.keys())\n",
    "DOC_RECORDS = [docs[i] for i in DOC_IDS]\n",
    "DOC_TOKENS = tokenize_documents(DOC_RECORDS)\n",
    "\n",
    "print(docs[1])         \n",
    "print(DOC_TOKENS[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183f40d",
   "metadata": {},
   "source": [
    "### Build vocabulary\n",
    "\n",
    "Create a set (or list) of unique terms from all tokenized documents. Report the number of unique terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9cc192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms: 6926\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a set or list of unique terms\n",
    "\n",
    "\n",
    "# Report: \n",
    "# - Number of unique terms\n",
    "\n",
    "unique_terms = sorted({t for toks in DOC_TOKENS for t in toks})\n",
    "print(f\"Number of unique terms: {len(unique_terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb02912",
   "metadata": {},
   "source": [
    "### Build inverted index\n",
    "\n",
    "For each term, store the list (or set) of document IDs where the term appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "393b2683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gas -> [18, 22, 27, 33, 34, 49, 68, 69, 73, 85]\n",
      "pressure -> [3, 10, 11, 14, 15, 16, 19, 20, 25, 27]\n",
      "aircraft -> [12, 14, 29, 47, 51, 75, 76, 78, 100, 172]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "appears_in = dict()\n",
    "# TODO: For each term, store list of document IDs where the term appears\n",
    "INDEX = defaultdict(set)\n",
    "for doc_id, toks in zip(DOC_IDS, DOC_TOKENS):\n",
    "    for t in set(toks):           # set() avoids duplicate adds from repeats in a doc\n",
    "        INDEX[t].add(doc_id)\n",
    "\n",
    "INDEX = {t: sorted(ids) for t, ids in INDEX.items()}\n",
    "\n",
    "# quick spot checks\n",
    "for t in [\"gas\", \"pressure\", \"aircraft\"]:\n",
    "    print(t, \"->\", INDEX.get(t, [])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e81bf",
   "metadata": {},
   "source": [
    "### Retrieve documents for a Boolean query (AND/OR)\n",
    "\n",
    "Create a function to retrieve documents for a Boolean query (AND/OR) with query terms.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c9318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function for retrieving documents for a Boolean query (AND/OR) with query terms\n",
    "\n",
    "from typing import Dict, Iterable, List, Set\n",
    "\n",
    "def boolean_retrieve(query: str, index: Dict[str, Iterable[int]]) -> List[int]:\n",
    "    def postings(term: str) -> Set[int]:\n",
    "        return set(index.get(term, []))\n",
    "\n",
    "    tokens = query.split()\n",
    "    # Split on OR, intersect within each group of ANDs, then union the groups.\n",
    "    groups: List[List[str]] = []\n",
    "    cur: List[str] = []\n",
    "    for tok in tokens:\n",
    "        u = tok.upper()\n",
    "        if u == \"OR\":\n",
    "            if cur:\n",
    "                groups.append(cur)\n",
    "                cur = []\n",
    "        elif u == \"AND\":\n",
    "            continue\n",
    "        else:\n",
    "            cur.append(tok.lower())\n",
    "    if cur:\n",
    "        groups.append(cur)\n",
    "\n",
    "    result: Set[int] = set()\n",
    "    for g in groups:\n",
    "        if not g:\n",
    "            continue\n",
    "        inter = postings(g[0])\n",
    "        for term in g[1:]:\n",
    "            inter &= postings(term)\n",
    "            if not inter:\n",
    "                break\n",
    "        result |= inter\n",
    "    return sorted(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf47585",
   "metadata": {
    "deletable": false,
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "boolean_queries = [\n",
    "  \"gas AND pressure\",\n",
    "  \"structural AND aeroelastic AND flight AND high AND speed OR aircraft\",\n",
    "  \"heat AND conduction AND composite AND slabs\",\n",
    "  \"boundary AND layer AND control\",\n",
    "  \"compressible AND flow AND nozzle\",\n",
    "  \"combustion AND chamber AND injection\",\n",
    "  \"laminar AND turbulent AND transition\",\n",
    "  \"fatigue AND crack AND growth\",\n",
    "  \"wing AND tip AND vortices\",\n",
    "  \"propulsion AND efficiency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf286d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 => [27, 49, 85, 101, 110]\n",
      "Q2 => [12, 14, 29, 47, 51]\n",
      "Q3 => [5, 399]\n",
      "Q4 => [1, 61, 244, 265, 342]\n",
      "Q5 => [118, 131]\n",
      "Q6 => []\n",
      "Q7 => [7, 9, 80, 89, 96]\n",
      "Q8 => []\n",
      "Q9 => [675]\n",
      "Q10 => [968]\n"
     ]
    }
   ],
   "source": [
    "# Run Boolean queries in batch, using the function you created\n",
    "def run_batch_boolean(queries, index):\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        res = boolean_retrieve(q, index)\n",
    "        results[f\"Q{i}\"] = res\n",
    "    return results\n",
    "\n",
    "boolean_results = run_batch_boolean(boolean_queries, INDEX)\n",
    "for qid, res in boolean_results.items():\n",
    "    print(qid, \"=>\", res[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b591b81",
   "metadata": {},
   "source": [
    "## Part 1.2 – TF-IDF Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed448f",
   "metadata": {},
   "source": [
    "\n",
    "$tf_{i,j} = \\text{Raw Frequency}$\n",
    "\n",
    "$idf_t = \\log\\left(\\frac{N}{df_t}\\right)$\n",
    "\n",
    "### Build document–term matrix (TF and IDF weights)\n",
    "\n",
    "Compute tf and idf using the formulas above and store the weights in a document–term matrix (rows = documents, columns = terms).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629e32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 1400, Terms: 6926\n",
      "TF shape: (1400, 6926) IDF shape: (6926,) TFIDF shape: (1400, 6926)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate the weights for the documents and the terms using tf and idf weighting. Put these values into a document–term matrix (rows = documents, columns = terms).\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "if \"unique_terms\" in globals():\n",
    "    TERMS = sorted(unique_terms)\n",
    "else:\n",
    "    TERMS = sorted({t for toks in DOC_TOKENS for t in toks})\n",
    "TERM2COL = {t: i for i, t in enumerate(TERMS)}\n",
    "\n",
    "N = len(DOC_TOKENS)\n",
    "V = len(TERMS)\n",
    "\n",
    "# TF: raw term counts per doc\n",
    "TF = np.zeros((N, V), dtype=float)\n",
    "for d, toks in enumerate(DOC_TOKENS):\n",
    "    cnt = Counter(toks)\n",
    "    for t, c in cnt.items():\n",
    "        TF[d, TERM2COL[t]] = float(c)\n",
    "\n",
    "# df: document frequency per term\n",
    "if \"INDEX\" in globals():\n",
    "    df = np.array([len(INDEX.get(t, [])) for t in TERMS], dtype=float)\n",
    "else:\n",
    "    df = np.zeros(V, dtype=float)\n",
    "    for t, j in TERM2COL.items():\n",
    "        # count docs where t appears at least once\n",
    "        df[j] = sum(1 for toks in DOC_TOKENS if t in set(toks))\n",
    "\n",
    "# IDF: log(N / df_t)\n",
    "IDF = np.log(N / df)\n",
    "\n",
    "# TF–IDF weights\n",
    "TFIDF = TF * IDF  # rows = documents, cols = TERMS\n",
    "\n",
    "print(f\"Docs: {N}, Terms: {V}\")\n",
    "print(\"TF shape:\", TF.shape, \"IDF shape:\", IDF.shape, \"TFIDF shape:\", TFIDF.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007cbf7",
   "metadata": {},
   "source": [
    "### Build TF–IDF document vectors\n",
    "\n",
    "From the matrix, build a TF–IDF vector for each document (consider normalization if needed for cosine similarity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654b0c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC_VECS shape: (1400, 6926)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Build TF–IDF document vectors from the document–term matrix\n",
    "DOC_NORM = np.linalg.norm(TFIDF, axis=1, keepdims=True)\n",
    "DOC_NORM[DOC_NORM == 0.0] = 1.0           # avoid divide-by-zero for empty rows\n",
    "DOC_VECS = TFIDF / DOC_NORM               # shape (N_docs, V_terms), unit length rows\n",
    "\n",
    "print(\"DOC_VECS shape:\", DOC_VECS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df320c",
   "metadata": {},
   "source": [
    "### Implement cosine similarity\n",
    "\n",
    "Implement a function to compute cosine similarity scores between a (tokenized) query and all documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44d2e7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Create a function for calculating the similarity score of all the documents by their relevance to query terms\n",
    "from numpy import argsort\n",
    "\n",
    "def tfidf_retrieve(query: str):\n",
    "    query = query.lower()\n",
    "    query = re.sub(r\"[^a-z]+\", \" \", query)\n",
    "    query_tokens = [t for t in query.split() if t and t not in _SANITIZED_STOPWORDS]\n",
    "    if not query_tokens:\n",
    "        return []\n",
    "    \n",
    "    cnt = Counter(query_tokens)\n",
    "    qvec = np.zeros((V,), dtype=float)\n",
    "    for t,c in cnt.items():\n",
    "        if t in TERM2COL:\n",
    "            qvec[TERM2COL[t]] = float(c) * IDF[TERM2COL[t]]\n",
    "    qnorm = np.linalg.norm(qvec)\n",
    "    if qnorm == 0.0:\n",
    "        return []\n",
    "    qhat = qvec / qnorm\n",
    "    scores = DOC_VECS @ qhat\n",
    "\n",
    "    idx = argsort(-scores)\n",
    "    if scores[idx[0]] == 0.0:\n",
    "        return []\n",
    "    ranked_doc_ids = [DOC_IDS[i] for i in idx if scores[i] > 0.0]\n",
    "    return ranked_doc_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4968750",
   "metadata": {
    "deletable": false,
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "tfidf_queries = [\n",
    "  \"gas pressure\",\n",
    "  \"structural aeroelastic flight high speed aircraft\",\n",
    "  \"heat conduction composite slabs\",\n",
    "  \"boundary layer control\",\n",
    "  \"compressible flow nozzle\",\n",
    "  \"combustion chamber injection\",\n",
    "  \"laminar turbulent transition\",\n",
    "  \"fatigue crack growth\",\n",
    "  \"wing tip vortices\",\n",
    "  \"propulsion efficiency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18861681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 => [169, 1286, 167, 185, 1003]\n",
      "Q2 => [12, 51, 746, 875, 884]\n",
      "Q3 => [399, 144, 485, 5, 181]\n",
      "Q4 => [368, 748, 638, 451, 1349]\n",
      "Q5 => [389, 118, 1187, 172, 173]\n",
      "Q6 => [974, 628, 397, 308, 635]\n",
      "Q7 => [418, 1264, 315, 272, 9]\n",
      "Q8 => [768, 726, 1196, 883, 884]\n",
      "Q9 => [1284, 433, 675, 1271, 288]\n",
      "Q10 => [968, 1328, 1380, 1092, 592]\n"
     ]
    }
   ],
   "source": [
    "# Run TF-IDF queries in batch (print top-5 results for each), using the function you created\n",
    "def run_batch_tfidf(queries):\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        res = tfidf_retrieve(q)\n",
    "        results[f\"Q{i}\"] = res\n",
    "    return results\n",
    "\n",
    "tfidf_results = run_batch_tfidf(tfidf_queries)\n",
    "\n",
    "for qid, res in tfidf_results.items():\n",
    "    print(qid, \"=>\", res[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0989101",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1.3 – Conceptual Questions\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "**1. What is the difference between data retrieval and information retrieval?**\n",
    "*Your answer here*\n",
    "\n",
    "**For the following scenarios, which approach would be suitable data retrieval or information retrieval? Explain your reasoning.** <br>\n",
    "1.a A clerk in pharmacy uses the following query: Medicine_name = Ibuprofen_400mg\n",
    "*Your answer here*\n",
    "\n",
    "1.b A clerk in pharmacy uses the following query: An anti-biotic medicine \n",
    "*Your answer here*\n",
    "\n",
    "1.c Searching for the schedule of a flight using the following query: Flight_ID = ZEFV2\n",
    "*Your answer here*\n",
    "\n",
    "1.d Searching an E-commerce website using the following query to find an specific shoe: Brooks Ghost 15\n",
    "*Your answer here*\n",
    "\n",
    "1.e Searching the same E-commerce website using the following query: Nice running shoes\n",
    "*Your answer here*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
